{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error = 0.010101010101 Lambda = 1.0\n",
      "Test error = 0.08333333333333333 \n",
      "\n",
      "Training error = 0.00505050505051 Lambda = 4.0\n",
      "Test error = 0.06818181818181818 \n",
      "\n",
      "Training error = 0.0126262626263 Lambda = 1.0\n",
      "Test error = 0.07575757575757576 \n",
      "\n",
      "Training error = 0.00757575757576 Lambda = 1.0\n",
      "Test error = 0.07575757575757576 \n",
      "\n",
      "Test error = 0.021645021645021644 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import pinv\n",
    "import math\n",
    "\n",
    "def read_data(datafile):\n",
    "    with open(datafile, 'r', encoding='utf-8-sig') as data_file:\n",
    "        data = []\n",
    "        for line in data_file:\n",
    "            lines = line.split()\n",
    "            lines = [float(i) for i in lines]\n",
    "            data.append(lines)\n",
    "        data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def split_data(data):\n",
    "    train = data[0:np.min(np.where(data[:,0]>0)),:]\n",
    "    test = data[np.min(np.where(data[:,0]>0)):len(data),:]\n",
    "    num_classes_train = int(np.max(train[:,len(train.T)-1])+1)\n",
    "    num_classes_test = int(np.max(test[:,len(train.T)-1])+1)\n",
    "    t_train = list(train[:,len(train.T)-1].astype(int))\n",
    "    t_test = list(test[:,len(train.T)-1].astype(int))\n",
    "    train = train[:,3:len(train.T)-1]\n",
    "    test = test[:,3:len(test.T)-1]\n",
    "    return train.T, test.T, num_classes_train, num_classes_test, t_train, t_test\n",
    "\n",
    "def split_training_data(train, t_train, i, subset_train):\n",
    "    validation_set = train[:,i*subset_train:(i+1)*subset_train]\n",
    "    t_train_valid = t_train[i*subset_train:(i+1)*subset_train]\n",
    "    train = np.delete(train, np.arange(i*subset_train,(i+1)*subset_train), 1)\n",
    "    t_train = np.delete(t_train, np.arange(i*subset_train,(i+1)*subset_train))\n",
    "    return validation_set, t_train_valid, train, t_train\n",
    "    \n",
    "def indices_to_one_hot(data, nb_classes):\n",
    "    return (np.eye(nb_classes)[np.array(data)]).T\n",
    "\n",
    "def compute_H_train(train):\n",
    "    samples = train.shape[1]\n",
    "    feature_length = train.shape[0] \n",
    "    nodes = 1000\n",
    "    W = np.random.normal(size=[feature_length, nodes])\n",
    "    B = np.random.normal(size=[nodes])\n",
    "    H_train = np.zeros((samples, nodes))\n",
    "    for i in range(nodes):\n",
    "        for j in range(samples):\n",
    "            H_train[j,i] = 1 / (1 + math.exp(-(np.matmul(W[:,i], train[:,j]) + B[i])))\n",
    "    return H_train, samples, nodes, W, B\n",
    "\n",
    "def compute_H_test(test, nodes, W, B):\n",
    "    samples = test.shape[1]\n",
    "    feature_length = test.shape[0]\n",
    "    H_test = np.zeros((samples, nodes))\n",
    "    for i in range(nodes):\n",
    "        for j in range(samples):\n",
    "            H_test[j,i] = 1 / (1 + math.exp(-(np.matmul(W[:,i], test[:,j]) + B[i])))\n",
    "    return H_test, samples\n",
    "\n",
    "def o_star_train(T, H, samples, nodes, num_classes):\n",
    "    min_lambda = 1\n",
    "    max_lambda = 10\n",
    "    length = int(max_lambda/min_lambda)\n",
    "    lambda_vec = np.linspace(min_lambda,max_lambda,length)\n",
    "    error = np.zeros((length))\n",
    "    o_star = np.zeros((nodes, num_classes, length))\n",
    "    W1 = np.random.normal(size=[nodes])\n",
    "    B1 = np.random.normal(size=[nodes])\n",
    "    H1 = np.zeros((samples,nodes))\n",
    "    W2 = np.random.normal(size=[nodes])\n",
    "    B2 = np.random.normal(size=[nodes])\n",
    "    H3 = np.zeros((samples,nodes))\n",
    "    if nodes >= samples:\n",
    "        for lam in range(length):\n",
    "            lam_mat = lambda_vec[lam]*np.eye(samples, samples)\n",
    "            for i in range(nodes):\n",
    "                H1[:,i] = 1 / (1 + np.exp(-(W1[i]*H[:,i] + B1[i])))\n",
    "            W_he = np.matmul(np.log(H1/(1-H1)), pinv(H))\n",
    "            H2 = 1 / (1 + np.exp(-np.matmul(W_he, H)))\n",
    "            for i in range(nodes):\n",
    "                H3[:,i] = 1 / (1 + np.exp(-(W2[i]*H2[:,i] + B2[i])))\n",
    "            W_he1 = np.matmul(np.log(H3/(1-H3)), pinv(H2))\n",
    "            H4 = 1 / (1 + np.exp(-np.matmul(W_he1, H2)))\n",
    "            o_star[:,:,lam] = np.matmul(np.matmul(H4.T, inv(np.matmul(H4, H4.T) + lam_mat)), T.T)\n",
    "            T_hat = (np.matmul(H4, o_star[:,:,lam])).T\n",
    "            error[lam] = compute_accuracy(T_hat, T, samples)\n",
    "        print(\"Training error =\", error[np.argmin(error)], \"Lambda =\", lambda_vec[np.argmin(error)])\n",
    "    if nodes < samples:\n",
    "        for lam in range(length):\n",
    "            lam_mat = lambda_vec[lam]*np.eye(nodes, nodes)\n",
    "            for i in range(nodes):\n",
    "                H1[:,i] = 1 / (1 + np.exp(-(W1[i]*H[:,i] + B1[i])))\n",
    "            W_he = np.matmul(np.log(H1/(1-H1)), pinv(H))\n",
    "            H2 = 1 / (1 + np.exp(-np.matmul(W_he, H)))\n",
    "            for i in range(nodes):\n",
    "                H3[:,i] = 1 / (1 + np.exp(-(W2[i]*H2[:,i] + B2[i])))\n",
    "            W_he1 = np.matmul(np.log(H3/(1-H3)), pinv(H2))\n",
    "            H4 = 1 / (1 + np.exp(-np.matmul(W_he1, H2)))\n",
    "            o_star[:,:,lam] = np.matmul(np.matmul(inv(np.matmul(H4.T, H4) + lam_mat), H4.T), T.T)\n",
    "            T_hat = (np.matmul(H4, o_star[:,:,lam])).T\n",
    "            error[lam] = compute_accuracy(T_hat, T, samples)\n",
    "        print(\"Training error =\", error[np.argmin(error)], \"Lambda =\", lambda_vec[np.argmin(error)])\n",
    "    return o_star[:,:,np.argmin(error)]\n",
    "\n",
    "def T_hat_test(H, o_star, T, samples):\n",
    "    T_hat_test = np.matmul(H, o_star).T\n",
    "    error = compute_accuracy(T_hat_test, T, samples)\n",
    "    print(\"Test error =\", error, \"\\n\")\n",
    "    return error\n",
    "\n",
    "def compute_accuracy(T_hat, T, samples):\n",
    "    class_matrix = np.eye(T.shape[0],T.shape[0])\n",
    "    for i in np.arange(samples):\n",
    "        normi = []\n",
    "        for k in np.arange(len(class_matrix)):\n",
    "            norma = norm(class_matrix[:,k]-T_hat[:,i], ord=2)**2\n",
    "            normi.append(norma)\n",
    "        normi = np.array([normi])\n",
    "        classi = np.argmin(normi)\n",
    "        T_hat[:,i] = class_matrix[:,classi]\n",
    "    wrongly_classified = T - T_hat\n",
    "    count = 0\n",
    "    for i in np.arange(T_hat.shape[0]):\n",
    "        if sum(wrongly_classified[:,i] > 0) > 0:\n",
    "            count = count + 1\n",
    "    error = count/samples\n",
    "    return error\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    data = read_data('vowel-context.txt')\n",
    "    train, test, num_classes_train, num_classes_test, t_train, t_test = split_data(data)\n",
    "    \n",
    "    \"\"\"\"\n",
    "    T_train = indices_to_one_hot(t_train, num_classes_train)\n",
    "    T_test = indices_to_one_hot(t_test, num_classes_train)\n",
    "    H_train, samples, nodes, W, B = compute_H_train(train)\n",
    "    o_star_train = o_star_train(T_train, H_train, samples, nodes, num_classes_train)\n",
    "    H_test, samples = compute_H_test(test, nodes, W, B)\n",
    "    T_hat_test(H_test, o_star_train, T_test, samples)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    k_folds = 4\n",
    "    subset_train = int(train.shape[1]/k_folds)\n",
    "    error = np.zeros((k_folds))\n",
    "    o_star_mat = []\n",
    "    for i in range(k_folds):\n",
    "        validation_set, t_train_valid, train_train, t_train_train = split_training_data(train, t_train, i, subset_train)\n",
    "        T_train = indices_to_one_hot(t_train_train, num_classes_train)\n",
    "        T_test = indices_to_one_hot(t_train_valid, num_classes_test)\n",
    "        H_train, samples, nodes, W, B = compute_H_train(train_train)\n",
    "        o_star = o_star_train(T_train, H_train, samples, nodes, num_classes_train)\n",
    "        o_star_mat.append(o_star)\n",
    "        H_test, samples = compute_H_test(validation_set, nodes, W, B)\n",
    "        error[i] = T_hat_test(H_test, o_star, T_test, samples)\n",
    "    o_star = o_star_mat[np.argmin(error)]\n",
    "    H_test, samples = compute_H_test(test, nodes, W, B)\n",
    "    T_test = indices_to_one_hot(t_test, num_classes_test)\n",
    "    T_hat_test(H_test, o_star, T_test, samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
